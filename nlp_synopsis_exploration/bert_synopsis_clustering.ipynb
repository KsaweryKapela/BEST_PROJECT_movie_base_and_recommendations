{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "sys.path.append(os.getenv('MAINDIR'))\n",
    "from main import MoviesDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis = [movie.description for movie in MoviesDatabase.query.all()]\n",
    "genres = [movie.genre for movie in MoviesDatabase.query.all()]\n",
    "titles = [movie.title for movie in MoviesDatabase.query.all()]\n",
    "collection = [movie.collection for movie in MoviesDatabase.query.all()]\n",
    "critics = [movie.computed_critic_score for movie in MoviesDatabase.query.all()]\n",
    "studio = [movie.studio for movie in MoviesDatabase.query.all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Imprisoned on the other side of the universe, the mighty Thor finds himself in a deadly gladiatorial contest that pits him against the Hulk, his former ally and fellow Avenger. Thor's quest for survival leads him in a race against time to prevent the all-powerful Hela from destroying his home world and the Asgardian civilization.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synopsis[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bert_transformer import BertTransformer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\").to('cuda:0')\n",
    "\n",
    "bert_transformer = BertTransformer(tokenizer, model, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectorized \u001b[39m=\u001b[39m bert_transformer\u001b[39m.\u001b[39;49mfit_transform([synopsis])\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Movie_Base_And_Recommendations/synopsis_clustering/bert_transformer.py:43\u001b[0m, in \u001b[0;36mBertTransformer.transform\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     40\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenize_and_predict(string) \u001b[39mfor\u001b[39;00m string \u001b[39min\u001b[39;00m text])\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/Desktop/Movie_Base_And_Recommendations/synopsis_clustering/bert_transformer.py:43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize_and_predict(string) \u001b[39mfor\u001b[39;00m string \u001b[39min\u001b[39;00m text])\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/Desktop/Movie_Base_And_Recommendations/synopsis_clustering/bert_transformer.py:32\u001b[0m, in \u001b[0;36mBertTransformer._tokenize_and_predict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tokenize_and_predict\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mtensor:\n\u001b[0;32m---> 32\u001b[0m     tokenized, attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(text)\n\u001b[1;32m     34\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert_model(tokenized[\u001b[39m0\u001b[39m], attention_mask)\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_func(embeddings)\n",
      "File \u001b[0;32m~/Desktop/Movie_Base_And_Recommendations/synopsis_clustering/bert_transformer.py:17\u001b[0m, in \u001b[0;36mBertTransformer._tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mtensor, torch\u001b[39m.\u001b[39mtensor]:\n\u001b[0;32m---> 17\u001b[0m     tokenized_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_tokenizer\u001b[39m.\u001b[39;49mencode_plus(text,\n\u001b[1;32m     18\u001b[0m                                                      add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     19\u001b[0m                                                      return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m                                                      max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m     21\u001b[0m                                                      )[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     23\u001b[0m     attention_mask \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(tokenized_text)\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m         torch\u001b[39m.\u001b[39mtensor(tokenized_text)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m     27\u001b[0m         torch\u001b[39m.\u001b[39mtensor(attention_mask)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m     28\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2781\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2771\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2772\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2773\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2774\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2778\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2779\u001b[0m )\n\u001b[0;32m-> 2781\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2782\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2783\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2784\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2785\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2786\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2787\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2788\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2789\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2790\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2791\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2792\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2793\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2794\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2795\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2796\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2797\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2798\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2799\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2800\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:517\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    496\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    497\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    515\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    516\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 517\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    518\u001b[0m         batched_input,\n\u001b[1;32m    519\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    520\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    521\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    522\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    523\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    524\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    525\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    526\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    527\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    528\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    529\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    530\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    531\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    532\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    533\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    534\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    535\u001b[0m     )\n\u001b[1;32m    537\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:445\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    438\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    439\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    443\u001b[0m )\n\u001b[0;32m--> 445\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    446\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    447\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    448\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    457\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    459\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    469\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "vectorized = bert_transformer.fit_transform([synopsis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 9887) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcluster\u001b[39;00m \u001b[39mimport\u001b[39;00m KMeans\n\u001b[0;32m----> 2\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans(\u001b[39m50\u001b[39;49m)\u001b[39m.\u001b[39;49mfit([vectorized, collection])\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1471\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1445\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \n\u001b[1;32m   1447\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1471\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1472\u001b[0m         X,\n\u001b[1;32m   1473\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1474\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m   1475\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1476\u001b[0m         copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[1;32m   1477\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1478\u001b[0m     )\n\u001b[1;32m   1480\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1482\u001b[0m     random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/sklearn/utils/validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/movie_env/lib/python3.10/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 9887) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(50).fit([vectorized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 45, 16, ..., 12,  6, 32], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9887,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.asarray(synopsis).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Cluster': kmeans.labels_, 'Synopsis': synopsis, 'Genres': genres, 'Title': titles, 'Collection': collection, 'Critics': critics, 'Studio': studio}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Studio\n",
       "                                                                    2779\n",
       "Paramount Pictures                                                   405\n",
       "Warner Bros. Pictures                                                391\n",
       "Universal Pictures                                                   377\n",
       "20th Century Fox                                                     373\n",
       "                                                                    ... \n",
       "Warner Bros. Pictures, MGM/UA Home Entertainment Inc.                  1\n",
       "Roadshow Home Video [au], Metro-Goldwyn-Mayer Distributing Corp.       1\n",
       "Brandon Films Inc., Cowboy Pictures                                    1\n",
       "Cinelou Films                                                          1\n",
       "Paramount+                                                             1\n",
       "Name: count, Length: 1077, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Studio.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360     Aaron answers an online ad and drives to a str...\n",
       "506     A unique and darkly engaging spiritual thrille...\n",
       "522     Being stranded on a deserted island leaves you...\n",
       "606     After a gentle alien becomes stranded on Earth...\n",
       "701     The Mayan kingdom is at the height of its opul...\n",
       "                              ...                        \n",
       "9322    Father Merrin (Stellan Skarsgard) is haunted b...\n",
       "9324    After his true love (Isabel Glasser) falls in ...\n",
       "9507    A less-than-successful exercise equipment sale...\n",
       "9795    After his ship sinks, Andrew Braddock (Michael...\n",
       "9829    Upon arriving at a remote cabin in the redwood...\n",
       "Name: Synopsis, Length: 162, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Cluster'] == 1].Synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81,\n",
       " {'Documentary': 53,\n",
       "  'Documentary, Music': 10,\n",
       "  'Biography': 4,\n",
       "  'Drama': 4,\n",
       "  'Comedy, Stand-up': 2},\n",
       " {'': 81},\n",
       " array(['Hannah Gadsby: Nanette', 'Dolemite Is My Name',\n",
       "        'Hot Girls Wanted', '22 July', 'Andy Irons: Kissed by God',\n",
       "        'Reversing Roe', \"Won't You Be My Neighbor?\", 'Sexy Baby',\n",
       "        'A Poem Is a Naked Person', 'A Most Beautiful Thing',\n",
       "        'Rolling Thunder Revue: A Bob Dylan Story by Martin Scorsese',\n",
       "        'Taylor Swift: Miss Americana', 'The Kingmaker',\n",
       "        'Rumble: The Indians Who Rocked the World',\n",
       "        'This Changes Everything', 'Grindhouse',\n",
       "        '3 1/2 Minutes, Ten Bullets', '13TH',\n",
       "        'The Man Nobody Knew: In Search of My Father, CIA Spymaster William Colby',\n",
       "        'This Is Elvis', 'The Act of Killing', 'A Quiet Passion',\n",
       "        'Bowling for Columbine', 'Jerrod Carmichael: Rothaniel', 'Mishima',\n",
       "        'The Grizzlies', 'Girl 27', 'The Plot Against the President',\n",
       "        'The Hurricane', 'The Trip to Greece', 'The Imposter',\n",
       "        'Between Me and My Mind', 'Room 237', 'A Thousand Cuts',\n",
       "        '#Unfit: The Psychology of Donald Trump', 'Hitchcock/Truffaut',\n",
       "        'After the End', 'We Steal Secrets: The Story of WikiLeaks',\n",
       "        'The Catcher Was a Spy', 'No Safe Spaces', 'Samsara',\n",
       "        'In Search of Greatness', \"The Hornet's Nest\", 'Spielberg',\n",
       "        \"Hillary's America: The Secret History of the Democratic Party\",\n",
       "        'Zeitgeist: The Movie', 'The Missing Picture',\n",
       "        \"David Byrne's American Utopia\",\n",
       "        'David Attenborough: A Life on Our Planet',\n",
       "        \"Film Stars Don't Die in Liverpool\",\n",
       "        'Going Clear: Scientology & the Prison of Belief', 'The King',\n",
       "        'Kurt Cobain: Montage of Heck', 'McQueen', 'Life Itself',\n",
       "        'An Inconvenient Truth', 'Intent to Destroy', 'Becoming',\n",
       "        'Capitalism: A Love Story', '20 Feet From Stardom', 'Ronaldo',\n",
       "        'Cartel Land', \"Robin's Wish\", 'Seymour: An Introduction',\n",
       "        'The Devil and Father Amorth', 'Janis: Little Girl Blue',\n",
       "        'Fahrenheit 9/11', 'For the Love of Spock', 'Red Dog',\n",
       "        'Amazing Grace', 'One More Time With Feeling', 'Cocaine Cowboys',\n",
       "        'De Palma', 'The Beatles: Eight Days a Week -- The Touring Years',\n",
       "        'Muscle Shoals', 'A Beautiful Planet', 'Crazy, Not Insane',\n",
       "        'The Kentucky Fried Movie', 'The Problem With Apu', 'Bully',\n",
       "        'Eating Animals'], dtype=object))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 18\n",
    "len(df[df['Cluster'] == n]), dict(df[df['Cluster'] == n].Genres.value_counts()[:5]), dict(df[df['Cluster'] == n].Collection.value_counts()[:5]), df[df['Cluster'] == n]['Title'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
