{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "sys.path.append(os.getenv('MAINDIR'))\n",
    "from main import MoviesDatabase\n",
    "\n",
    "synopsis = [movie.description for movie in MoviesDatabase.query.all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4.0, 'fuckin awesome'), (1.0, 'im')]\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text('Im so fuckin awesome!')\n",
    "print(r.get_ranked_phrases_with_scores())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "\n",
    "for syno in synopsis:\n",
    "    r.extract_keywords_from_text(syno)\n",
    "    keywords.append(r.get_ranked_phrases_with_scores()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'keywords': keywords, 'synopsis': synopsis})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Batman ventures into Gotham City's underworld when a sadistic killer leaves behind a trail of cryptic clues. As the evidence begins to lead closer to home and the scale of the perpetrator's plans become clear, he must forge new relationships, unmask the culprit and bring justice to the abuse of power and corruption that has long plagued the metropolis.\",\n",
       " [(16.0, 'sadistic killer leaves behind'),\n",
       "  (16.0, 'must forge new relationships'),\n",
       "  (9.0, 'plans become clear')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 11\n",
    "df.iloc[n]['synopsis'], df.iloc[n].keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('relationships', 1)\n",
      "('trail', 1)\n",
      "('culprit', 1)\n",
      "('corruption', 1)\n",
      "('killer', 1)\n",
      "('abuse', 1)\n",
      "('scale', 1)\n",
      "('city', 1)\n",
      "('sadistic', 1)\n",
      "('home', 1)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_hotwords(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] \n",
    "    doc = nlp(text.lower()) \n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "    return result\n",
    "\n",
    "output = set(get_hotwords(df.iloc[n]['synopsis']))\n",
    "most_common_list = Counter(output).most_common(10)\n",
    "for item in most_common_list:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gotham City underworld', 0.0003156958886588181)\n",
      "('sadistic killer leaves', 0.0010378185479078276)\n",
      "('Gotham City', 0.0031146356952437464)\n",
      "('ventures into Gotham', 0.00561955076575256)\n",
      "('City underworld', 0.00561955076575256)\n",
      "('Batman ventures', 0.007948096222458798)\n",
      "('cryptic clues', 0.007948096222458798)\n",
      "('sadistic killer', 0.010159422250580143)\n",
      "('killer leaves', 0.010159422250580143)\n",
      "('trail of cryptic', 0.010159422250580143)\n",
      "('Gotham', 0.05572221155747183)\n",
      "('City', 0.05572221155747183)\n",
      "('plans become clear', 0.06815467895249562)\n",
      "('forge new relationships', 0.06815467895249562)\n",
      "('unmask the culprit', 0.06815467895249562)\n",
      "('plagued the metropolis', 0.06815467895249562)\n",
      "('Batman', 0.07862947958994491)\n",
      "('clues', 0.07862947958994491)\n",
      "('evidence begins', 0.08446611338711053)\n",
      "('begins to lead', 0.08446611338711053)\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(df.iloc[n]['synopsis'])\n",
    "for kw in sorted(keywords, key=lambda x: x[1]):\n",
    "  print(kw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksaff/miniconda3/envs/movie_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gotham', 0.5918), ('batman', 0.56), ('underworld', 0.3728), ('metropolis', 0.296), ('city', 0.286)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(df.iloc[n]['synopsis'])\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = kw_model.extract_keywords(synopsis, keyphrase_ngram_range=(1, 1), top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thor', 0.6371), ('hulk', 0.503), ('asgardian', 0.4379), ('hela', 0.3617), ('imprisoned', 0.3391), ('avenger', 0.3303), ('universe', 0.3156), ('gladiatorial', 0.2522), ('quest', 0.244), ('mighty', 0.2251)]\n"
     ]
    }
   ],
   "source": [
    "print(keywords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "\n",
    "for keylist in keywords:\n",
    "    only_words = [word[0] for word in keylist]\n",
    "    all_keywords += only_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16646\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_keywords = [ps.stem(word) for word in set(all_keywords)]\n",
    "print(len(set(stemmed_keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ksaff/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from stemming.porter2 import stem\n",
    "from stemming.lovins import stem as lovins_stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "stemmed_keywords = [lemmatizer.lemmatize(word) for word in all_keywords]\n",
    "print(len(set(stemmed_keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('babynames.csv')\n",
    "namelist = set(df['Name'])\n",
    "\n",
    "no_names = [word for word in stemmed_keywords if word.capitalize() not in namelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts=pd.Series(no_names).value_counts()\n",
    "filtered_words = value_counts[value_counts > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4761"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = np.array(filtered_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ksaff/Desktop/Movie_Base_And_Recommendations/nlp_synopsis_exploration/bert_transformer.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(tokenized_text).unsqueeze(0).to(self.device),\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bert_transformer import BertTransformer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\").to('cuda:0')\n",
    "\n",
    "bert_transformer = BertTransformer(tokenizer, model, 500)\n",
    "vectorized = bert_transformer.fit_transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1541\n",
      "4097\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_1 = vectorized[0]\n",
    "for i in range(len(vectorized[1:])):\n",
    "    if torch.allclose(tensor_1, vectorized[i + 1], rtol=50):\n",
    "        print(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creation'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[4097]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(500).fit(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Cluster': kmeans.labels_, 'Word': words}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>101</td>\n",
       "      <td>road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>101</td>\n",
       "      <td>farm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>101</td>\n",
       "      <td>rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>101</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>101</td>\n",
       "      <td>cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>101</td>\n",
       "      <td>plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>101</td>\n",
       "      <td>refuge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>101</td>\n",
       "      <td>feudal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>101</td>\n",
       "      <td>mill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>101</td>\n",
       "      <td>repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>101</td>\n",
       "      <td>construction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>101</td>\n",
       "      <td>racetrack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>101</td>\n",
       "      <td>elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>101</td>\n",
       "      <td>pond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158</th>\n",
       "      <td>101</td>\n",
       "      <td>contractor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>101</td>\n",
       "      <td>convoy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cluster          Word\n",
       "310       101          road\n",
       "322       101          farm\n",
       "597       101         rural\n",
       "829       101      building\n",
       "1394      101          cell\n",
       "1426      101         plant\n",
       "1533      101        refuge\n",
       "1916      101        feudal\n",
       "2149      101          mill\n",
       "2838      101        repair\n",
       "2944      101  construction\n",
       "3031      101     racetrack\n",
       "3291      101    elementary\n",
       "3634      101          pond\n",
       "4158      101    contractor\n",
       "4667      101        convoy"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Cluster'] == 101]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
